{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiction\n",
    "\n",
    "Classification is used to predict discrete values.\n",
    "\n",
    "Examples include:\n",
    "- Email: Spam/spam\n",
    "- Online transactions: Frandaulent (Yes/No)\n",
    "- Tumor: Malignant/Benign\n",
    "\n",
    "There is multiclass and binary class classification.\n",
    "\n",
    "![title](img/image_68.png)\n",
    "\n",
    "You could use linear regression to predict 1 or 0, but this has its shortcomings.\n",
    "\n",
    "\n",
    "![title](img/image_69.png)\n",
    "\n",
    "Outliers can create issues with this approach.\n",
    "\n",
    "![title](img/image_70.png)\n",
    "\n",
    "Applying linear regression to a classification problem is not a good idea.\n",
    "\n",
    "Moreover, if you provide a training set with only values 0 and 1, the hypothesis function may output values greater than 1 or less than 0.\n",
    "\n",
    "![title](img/image_71.png)\n",
    "\n",
    "This seems odd.\n",
    "\n",
    "![title](img/image_72.png)\n",
    "\n",
    "\n",
    "This is why we use logistic regression, where y is constrained to be between 0 and 1:\n",
    "![title](img/image_73.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Representation for Logistic Regression\n",
    "\n",
    "The logistic regression function takes the form of:\n",
    "\n",
    "![title](img/image_74.png)\n",
    "\n",
    "### Interpretation of Hypothesis Output\n",
    "\n",
    "Hypothesis function outputs the estimated probability that the input x is = 1.\n",
    "\n",
    "![title](img/image_75.png)\n",
    "\n",
    "![title](img/image_76.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary\n",
    "\n",
    "\n",
    "![title](img/image_77.png)\n",
    "\n",
    "![title](img/image_78.png)\n",
    "\n",
    "![title](img/image_79.png)\n",
    "\n",
    "## Non-linear decision boundaries\n",
    "\n",
    "![title](img/image_80.png)\n",
    "\n",
    "## More complex decision boundaries\n",
    "\n",
    "![title](img/image_81.png)\n",
    "\n",
    "\n",
    "## Cost function for logistic regression\n",
    "\n",
    "![title](img/image_82.png)\n",
    "\n",
    "![title](img/image_83.png)\n",
    "\n",
    "![title](img/image_84.png)\n",
    "\n",
    "![title](img/image_85.png)\n",
    "\n",
    "![title](img/image_86.png)\n",
    "\n",
    "## Combine conditional cost functions\n",
    "\n",
    "![title](img/image_87.png)\n",
    "\n",
    "More clearly:\n",
    "\n",
    "![title](img/image_88.png)\n",
    "\n",
    "This specific cost function can be derived from the principal of maxiumum likelyhood estimation - which allows to estimate theta efficeintly for different models.\n",
    "\n",
    "## Gradient descent function for logistic cost function\n",
    "\n",
    "![title](img/image_89.png)\n",
    "\n",
    "![title](img/image_90.png)\n",
    "\n",
    "## Hypothesis function is different\n",
    "\n",
    "![title](img/image_91.png)\n",
    "\n",
    "![title](img/image_92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Advanced Optimization\n",
    "\n",
    "![title](img/image_93.png)\n",
    "\n",
    "![title](img/image_94.png)\n",
    "\n",
    "- Good and bad implementations of these advanced optimization algorithms exist, this specialty is numerical computing.\n",
    "\n",
    "### Example\n",
    "\n",
    "![title](img/image_94.png)\n",
    "\n",
    "![title](img/image_95.png)\n",
    "\n",
    "![title](img/image_96.png)\n",
    "\n",
    "![title](img/image_97.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "![title](img/image_98.png)\n",
    "\n",
    "\n",
    "One vs all is an approach to classify multiple sets\n",
    "\n",
    "![title](img/image_99.png)\n",
    "\n",
    "![title](img/image_100.png)\n",
    "\n",
    "![title](img/image_101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The problem of overfitting\n",
    "\n",
    "![title](img/image_102.png)\n",
    "\n",
    "![title](img/image_103.png)\n",
    "\n",
    "![title](img/image_104.png)\n",
    "\n",
    "![title](img/image_105.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Regularization\n",
    "\n",
    "![title](img/image_106.png)\n",
    "\n",
    "![title](img/image_107.png)\n",
    "\n",
    "![title](img/image_108.png)\n",
    "\n",
    "![title](img/image_109.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
