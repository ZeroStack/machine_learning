{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "Last week we looked at linear regression with 1 variable. We will now investiage the idea of using more variables.\n",
    "\n",
    "This requires a new notation \n",
    "\n",
    "![title](img/image_49.png)\n",
    "\n",
    "This also means the hypothesis function needs to be expanded to accommodate for more predictor variables\n",
    "\n",
    "![title](img/image_50.png)\n",
    "\n",
    "It is possible to represent all these values in the form of matrices.\n",
    "\n",
    "![title](img/image_51.png)\n",
    "\n",
    "\n",
    "![title](img/image_52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "The scale/ranges of the predictor variables matters when it comes to implementing gradient descent. Especially if the variables do not share a similar scale\n",
    "\n",
    "If features are on the same scale, by dividing each vector by the max, gradient descent will have an easier time reaching the local minimum. The scales effect the contour of the plots.\n",
    "\n",
    "![title](img/image_53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This raises an internal question, does scaling the features have any effect on the accuracy of the hypothesis function?\n",
    "\n",
    "![title](img/image_54.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mean normalization\n",
    "\n",
    "As opposed to dividing by the max value, another approach to scaling is called mean normalization. THis is when the original value is replaced by x(observation) - mean and then divided by the standard deviation\n",
    "\n",
    "![title](img/image_55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Debugging' Gradient Descent\n",
    "\n",
    "If gradient descent is working well:\n",
    "    \n",
    "![title](img/image_56.png)\n",
    "\n",
    "A too large learning rate can cause gradient descent to overshoot the minimum.\n",
    "\n",
    "![title](img/image_57.png)\n",
    "\n",
    "This large learning rate can also take the effect of \n",
    "\n",
    "![title](img/image_58.png)\n",
    "\n",
    "Intuitively, for a sufficiently small learning rate, the cost function should decrease on every iteration.\n",
    "![title](img/image_59.png)\n",
    "\n",
    "\n",
    "![title](img/image_60.png)\n",
    "\n",
    "When choosing a starting value of the learning rate\n",
    "\n",
    "![title](img/image_61.png)\n",
    "\n",
    "## How to express the hypothesis function as a polynomial regression?\n",
    "\n",
    "![title](img/image_62.png)\n",
    "\n",
    "When polynomial regression is used, it is important to consider feature scaling, as value of x, x^2 and x^3 will vary greatly.\n",
    "\n",
    "Example of a feature size:\n",
    "\n",
    "![title](img/image_63.png)\n",
    "\n",
    "![title](img/image_64.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Normal Equation\n",
    "\n",
    "![title](img/image_65.png)\n",
    "\n",
    "One challenge with using the normal equation method, that is, when you take the derivative (to find the rate a function for the rate of change and set it to 0), is when the matrix is non-invertible.\n",
    "\n",
    "## What is an invertible matrix?\n",
    "\n",
    "A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.\n",
    "\n",
    "## What if X<sup>T</sup>X is non-invertible?\n",
    "\n",
    "![title](img/image_66.png)\n",
    "\n",
    "## Octave Implementation of Normal Equation\n",
    "\n",
    "In octaive, you can use the 'pinv' function which give you the value of $ \\theta $ even if X<sup>T</sup> is non-invertible as opposed to the 'inv' function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
